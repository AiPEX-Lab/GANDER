{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7f7468710d90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "from torch import optim\n",
    "from torch.nn import functional as F \n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"airplane\",\"bench\",\"bowl\",\"cone\",\"desk\",\"flower_pot\",\n",
    "         \"keyboard\",\"mantel\",\"person\",\"radio\",\"sofa\",\"table\",\n",
    "         \"tv_stand\",\"xbox\",\"bathtub\",\"bookshelf\",\"car\",\"cup\",\n",
    "         \"door\",\"glass_box\",\"lamp\",\"monitor\",\"piano\",\"range_hood\",\n",
    "         \"stairs\",\"tent\",\"vase\",\"bed\",\"bottle\",\"chair\",\"curtain\",\n",
    "         \"dresser\",\"guitar\",\"laptop\",\"night_stand\",\"plant\",\n",
    "         \"sink\",\"stool\",\"toilet\",\"wardrobe\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "class MeshSampler(Sampler):\n",
    "    def __init__(self,dataset,batch_size=32):\n",
    "        self.epoch_size = len(dataset)\n",
    "        self.batch_size = batch_size\n",
    "        self.sample_ind = range(len(dataset))\n",
    "    def __len__(self):\n",
    "        return (self.epoch_size + self.batch_size - 1) // self.batch_size\n",
    "    def __iter__(self):\n",
    "        order = range(int(self.epoch_size/self.batch_size))\n",
    "        shuffle(order)\n",
    "        for i in order:\n",
    "            start = i * self.batch_size\n",
    "            end = min(start + self.batch_size, self.epoch_size)\n",
    "            yield self.sample_ind[start:end]\n",
    "from numpy import cross, eye, dot\n",
    "from scipy.linalg import expm, norm\n",
    "\n",
    "def renorm(data):\n",
    "    print data.shape\n",
    "    med = (np.max(data,axis=0) + np.min(data,axis=0))/2\n",
    "    data -= med\n",
    "    print med,np.max(np.linalg.norm(data,axis=1))\n",
    "    data /= np.max(np.linalg.norm(data,axis=1))\n",
    "    return data\n",
    "\n",
    "def M(axis, theta):\n",
    "    return expm(cross(eye(3), axis/norm(axis)*theta))\n",
    "def augment(faces,theta,rotate):\n",
    "    theta = theta*30*np.pi/180.\n",
    "    if rotate and theta > 0:\n",
    "        R = M([0,0,1],theta)\n",
    "        faces = np.dot(faces,R)\n",
    "    return faces\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "class MeshData(Dataset):\n",
    "    def __init__(self,root_directory, class_we_want, max_faces = 8000, max_vertices=5000):\n",
    "        print \"loading...\"\n",
    "        self.root = root_directory\n",
    "        self.df = pd.read_pickle(\"sizes.pkl\")\n",
    "        print \"done\"\n",
    "        self.df = self.df[(self.df.sizef<max_faces) &\n",
    "                          (self.df.sizev<max_vertices) &\n",
    "                          (self.df.cls_name==class_we_want) & \n",
    "                            (self.df.fname.str.contains(\"_0_\"))]#.sort_values(\"sizef\",ascending=False)\n",
    "        self.max_faces = np.max(self.df.sizef.values)\n",
    "        self.max_vertexes = np.max(self.df.sizev.values)\n",
    "    def __len__(self):\n",
    "        return len(self.df)*12\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx//12\n",
    "        theta = idx%12\n",
    "        fname = self.df.iloc[i]['fname']\n",
    "        end = fname.split(\"/\")[-1]\n",
    "        ftype = \"_\".join(fname.split(\"/\")[-1].split(\"_\")[:-4])\n",
    "        fno = fname.split(\"_\")[-1][:-4]\n",
    "        #fname = \"{}{}_{}_0.pkl\".format(self.root,ftype,fno)\n",
    "        data = pickle.load(open(fname,'rb'))\n",
    "        faces = data['faces']\n",
    "        vertices = data['vertices']\n",
    "        faces = faces.reshape(-1,3)\n",
    "        min_dist = np.min(np.linalg.norm(faces-vertices[0],axis=1))\n",
    "        assert min_dist < 0.0001, \"{}\".format(min_dist)\n",
    "        vertices = augment(vertices,theta,True)\n",
    "        faces = augment(faces,theta,True)\n",
    "        min_dist = np.min(np.linalg.norm(faces-vertices[0],axis=1))\n",
    "        assert min_dist < 0.0001, \"{}\".format(min_dist)\n",
    "        faces = faces.reshape(-1,9)\n",
    "        assert len(faces) <= self.max_faces\n",
    "        assert len(vertices) <= self.max_vertexes\n",
    "        return torch.from_numpy(vertices).float(),torch.from_numpy(faces).float()\n",
    "trainer = MeshData(\"../CleanedModels/pickle/\",\"chair\",3000,3000)\n",
    "\n",
    "def fill_with_zeros(x,l):\n",
    "    new_x = torch.zeros(l)\n",
    "    new_x[:x.size(0)]=x\n",
    "    return new_x\n",
    "    \n",
    "def pad_packer(x):\n",
    "    faces,cls_idx = zip(*x)\n",
    "    cls_idx = torch.cat(cls_idx)\n",
    "    max_size = faces[0].size()\n",
    "    lengths = [len(i) for i in faces]\n",
    "    faces = [fill_with_zeros(i,max_size) for i in faces]\n",
    "    faces = Variable(torch.stack(faces,0))\n",
    "    return faces,cls_idx,lengths\n",
    "train_loader = DataLoader(trainer,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(x,y):\n",
    "    now = time.time()\n",
    "    x=x.view(3,1,3)\n",
    "    yd = torch.stack([y]*3).squeeze()\n",
    "    dist = yd-x\n",
    "    dist = torch.sum(dist.pow_(2),dim=2)\n",
    "    #dist[i,j] = distance from ith input to jth possibility\n",
    "    #print dist[i,j],exdist\n",
    "    math_time = time.time() - now\n",
    "    k = 3\n",
    "    final_ps = [None,None,None]\n",
    "    \n",
    "    now = time.time()\n",
    "    mindist,idxs = torch.topk(dist,k,dim=1,largest=False)\n",
    "    cnt = torch.arange(0,mindist.numel()).view_as(mindist).view(-1).cpu().numpy()\n",
    "    idxs,mindist=idxs.view(-1).cpu().data.numpy(),mindist.view(-1).cpu().data.numpy()\n",
    "    order = mindist.argsort()[::-1]\n",
    "    d = mindist[order]\n",
    "    distances = torch.FloatTensor([0,0,0])\n",
    "    pts = cnt[order]\n",
    "    lows = idxs[order]\n",
    "    order_time = time.time() - now\n",
    "    \n",
    "    now = time.time()\n",
    "    for i in range(len(cnt)):\n",
    "        pt = int(pts[i])//k\n",
    "        lowest = lows[i]\n",
    "        non_nones = [p for p in final_ps if p is not None]\n",
    "        if final_ps[pt] is not None:\n",
    "            continue\n",
    "        if lowest in non_nones:\n",
    "            continue\n",
    "        if final_ps[pt] is None:\n",
    "            distances[pt] = float(d[i])\n",
    "            final_ps[pt] = lowest\n",
    "        if None not in final_ps:\n",
    "            break\n",
    "    #print final_ps\n",
    "    for i in range(3):\n",
    "        final_ps[i]=y[final_ps[i]]\n",
    "    sort_time = time.time() - now\n",
    "    \n",
    "    now = time.time()\n",
    "    final_ps = torch.stack(final_ps)\n",
    "    \n",
    "    distances = torch.sqrt(torch.sum(torch.pow(final_ps-x.squeeze(),2),dim=1))\n",
    "    var_time = time.time() - now\n",
    "    \n",
    "    #print \"math: {}, order: {}, sorting: {}, var: {}\".format(math_time,order_time,sort_time,var_time)\n",
    "    \n",
    "    return final_ps,torch.mean(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.4082  0.4704  0.0607  0.4130  0.5701  0.5928  0.2203  0.6950  0.7860\n",
       " [torch.FloatTensor of size 1x9], Variable containing:\n",
       "  0.7038\n",
       " [torch.FloatTensor of size 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "#print [i for i in itertools.permutations([0,1,2], 3)]\n",
    "def one_dim_order_invariant(x,y):\n",
    "    # x = [3 x 3]\n",
    "    # y = [3 x 3]\n",
    "    #print x.size(),y.size()\n",
    "    d = 10\n",
    "    for p in itertools.permutations([0,1,2], 3):\n",
    "        td = 0\n",
    "        temp = torch.zeros_like(x)\n",
    "        for i,c in enumerate(p):\n",
    "            temp[i] = x[c]-y[i]\n",
    "        #print torch.norm(temp,2,0,True).size()\n",
    "        td = torch.sum(torch.norm(temp,2,0))\n",
    "        #print td\n",
    "        if td < d:\n",
    "            #print \"td: {}\".format(td)\n",
    "            d = td\n",
    "    assert d < 10\n",
    "    return d\n",
    "    \n",
    "def two_dim_order_invariant(x,y):\n",
    "    '''find the closest face in y to a given face x'''\n",
    "    # x = [3 x 3]\n",
    "    # y = [n x 3 x 3]\n",
    "    x,y = x.view(3,3),y.view(-1,3,3)\n",
    "    xx = torch.stack([x]*6)\n",
    "    #print xx[0]\n",
    "    for i,p in enumerate(itertools.permutations([0,1,2], 3)):\n",
    "        for j,c in enumerate(p):\n",
    "            xx[i,j] = x[c]\n",
    "    \n",
    "    yy = y.unsqueeze(1).expand(-1,6,-1,-1)\n",
    "    zz = yy-xx\n",
    "    pp = torch.sum(torch.norm(zz,2,-1),-1)\n",
    "    pp,_ = torch.min(pp,1)\n",
    "    d,idx = torch.min(pp,0)\n",
    "    best_face = y[idx]\n",
    "    return best_face.view(1,9),d\n",
    "\n",
    "#two_dim_order_invariant(Variable(torch.rand(3,3)),Variable(torch.rand(200,9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('LSTM') != -1:\n",
    "        nn.init.xavier_normal(m.weight_ih_l0.data)\n",
    "        nn.init.xavier_normal(m.weight_hh_l0.data)\n",
    "        nn.init.constant(m.bias_ih_l0.data,0.001)\n",
    "        nn.init.constant(m.bias_hh_l0.data,0.001)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal(m.weight.data)\n",
    "        nn.init.constant(m.bias.data,0.001)\n",
    "\n",
    "class VertexTranslationEncoder(nn.Module):\n",
    "    def __init__(self,enc_lstm_units,in_dims=3,bi=True):\n",
    "        super(VertexTranslationEncoder, self).__init__()\n",
    "        self.hidden_units = enc_lstm_units\n",
    "        self.lstm = nn.LSTM(3, self.hidden_units,bidirectional=bi)\n",
    "    def forward(self,vertexes,hidden):\n",
    "        vertexes = vertexes.view(1,1,-1)\n",
    "        return self.lstm(vertexes,hidden)\n",
    "\n",
    "class VertexTranslationDecoderWithAttention(nn.Module):\n",
    "    def __init__(self,dec_lstm_units,max_length=1000,out_dims=9,bi=True):\n",
    "        super(VertexTranslationDecoderWithAttention, self).__init__()\n",
    "        self.dec_hidden_size = dec_lstm_units\n",
    "        self.output_size = out_dims\n",
    "        self.max_length = max_length\n",
    "        self.dec_attn = nn.Linear(self.dec_hidden_size + self.output_size, self.max_length)\n",
    "        self.dec_attn_combine = nn.Linear(self.dec_hidden_size + self.output_size, self.dec_hidden_size)\n",
    "        self.dec_lstm = nn.LSTM(self.dec_hidden_size, self.dec_hidden_size)\n",
    "        self.out = nn.Linear(self.dec_hidden_size, self.output_size)\n",
    "        self.attn_time = 0\n",
    "        self.lstm_time = 0\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.view(1, 1, -1)\n",
    "        #print input.size(),hidden[0].size()\n",
    "        x = torch.cat((input[0], hidden[0][0]), 1)\n",
    "        #print x.size()\n",
    "        attn_weights = F.softmax(self.dec_attn(x),dim=1)\n",
    "        assert torch.sum(attn_weights).data[0] < 1.1, \"{}\".format(torch.sum(attn_weights).data[0])\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "\n",
    "        output = torch.cat((input[0], attn_applied[0]), 1)\n",
    "        output = self.dec_attn_combine(output).unsqueeze(0)\n",
    "        output, hidden = self.dec_lstm(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2148, 9]) torch.Size([2148, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_once(input_variable,target_variable,encoder,decoder,e_o,d_o,max_length=1000):\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    e_o.zero_grad()\n",
    "    d_o.zero_grad()\n",
    "    encoder_hidden = None\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, decoder.dec_hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "    decoder_input = Variable(torch.FloatTensor([[0,0,0,0,0,0,0,0,0]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = [e.view(1,1,-1) for e in encoder_hidden]\n",
    "    choices = input_variable.squeeze()\n",
    "    vert_loss = 0\n",
    "    face_loss = 0\n",
    "    #two types of losses:\n",
    "    # loss of generating vertex not from the list\n",
    "    # loss of generating face not from list\n",
    "    snap_time = 0\n",
    "    decode_time = 0\n",
    "    face_time = 0\n",
    "    raw_faces = []\n",
    "    snapped_faces = []\n",
    "    fixed_faces = []\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        snapped_face,d = find_nearest(decoder_output,choices)\n",
    "        vert_loss += d\n",
    "        tg, d = two_dim_order_invariant(snapped_face,target_variable)\n",
    "        face_loss += d\n",
    "        fixed_faces.append(tg)\n",
    "        snapped_faces.append(snapped_face)\n",
    "        raw_faces.append(decoder_output)\n",
    "        decoder_input = tg  # Teacher forcing\n",
    "        #print \"decode: {}, snap: {}, face: {}\".format(decode_time,snap_time,face_time)\n",
    "    loss = face_loss+vert_loss\n",
    "    loss.backward()\n",
    "    snapped_faces = torch.cat(snapped_faces)\n",
    "    fixed_faces = torch.cat(fixed_faces)\n",
    "    raw_faces = torch.cat(raw_faces)\n",
    "    print fixed_faces.size(),target_variable.size()\n",
    "    # truth to predicted loss\n",
    "    return\n",
    "    if global_counter % genevery == 0:\n",
    "        write_off(fixed_faces.data.cpu().numpy(),\"genobjs/fixed_{}.off\".format(global_counter))\n",
    "        write_off(snapped_faces.data.cpu().numpy(),\"genobjs/snapped_{}.off\".format(global_counter))\n",
    "        write_off(raw_faces.data.cpu().numpy(),\"genobjs/raw_{}.off\".format(global_counter))\n",
    "    clip = 0.25\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "#     e_o.step()\n",
    "#     d_o.step()\n",
    "    return face_loss,vert_loss\n",
    "train_once(vertices,faces,encoder,decoder,encoder_optimizer,decoder_optimizer,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dims = 200\n",
    "max_length = max(trainer.max_faces,trainer.max_vertexes)\n",
    "encoder = VertexTranslationEncoder(h_dims)\n",
    "decoder = VertexTranslationDecoderWithAttention(h_dims*2,max_length=max_length)\n",
    "if use_cuda:\n",
    "    encoder,decoder = encoder.cuda(),decoder.cuda()\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(),lr=0.0001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard things\n",
    "writer = SummaryWriter()\n",
    "global_counter = 0 \n",
    "\n",
    "#vandf.load_state_dict(torch.load(\"facer.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 3543\n",
      "9 3543\n",
      "186 3543\n",
      "6 234\n",
      "5 234\n",
      "126 234\n",
      "6 1800\n",
      "10 1800\n",
      "146 1800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-8e025a40daf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print faces.size()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         face_loss,vert_loss = train_once(vertices,faces,encoder,decoder,\n\u001b[0;32m---> 13\u001b[0;31m                                          encoder_optimizer,decoder_optimizer,max_length)\n\u001b[0m\u001b[1;32m     14\u001b[0m         writer.add_scalars(\"Losses\",{\"snapping\":vert_loss.data[0]/target_length,\n\u001b[1;32m     15\u001b[0m                                      \"faceloss\":face_loss.data[0]/target_length},global_counter)\n",
      "\u001b[0;32m<ipython-input-42-3d4907eac0e0>\u001b[0m in \u001b[0;36mtrain_once\u001b[0;34m(input_variable, target_variable, encoder, decoder, e_o, d_o, max_length)\u001b[0m\n\u001b[1;32m     34\u001b[0m         decoder_output, decoder_hidden, decoder_attention = decoder(\n\u001b[1;32m     35\u001b[0m             decoder_input, decoder_hidden, encoder_outputs)\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0msnapped_face\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_nearest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mvert_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_dim_order_invariant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnapped_face\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a5fdf129d638>\u001b[0m in \u001b[0;36mfind_nearest\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mfinal_ps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_ps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_ps\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mvar_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "genevery = 10\n",
    "import time\n",
    "for i in range(30):\n",
    "    for idx,(vertices,faces) in enumerate(train_loader):\n",
    "        vertices,faces = vertices.view(-1,1,3),faces.view(-1,1,9)\n",
    "        vertices,faces = Variable(vertices),Variable(faces)\n",
    "        target_length = faces.size(0)\n",
    "        if use_cuda:\n",
    "            faces=faces.cuda()\n",
    "            vertices=vertices.cuda()\n",
    "        #print faces.size()\n",
    "        face_loss,vert_loss = train_once(vertices,faces,encoder,decoder,\n",
    "                                         encoder_optimizer,decoder_optimizer,max_length)\n",
    "        writer.add_scalars(\"Losses\",{\"snapping\":vert_loss.data[0]/target_length,\n",
    "                                     \"faceloss\":face_loss.data[0]/target_length},global_counter)\n",
    "        global_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py2.7/lib/python2.7/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type VertexTranslationEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/pytorch-py2.7/lib/python2.7/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type VertexTranslationDecoderWithAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(encoder,\"encoder.th\")\n",
    "torch.save(decoder,\"decoder.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def write_off(data,fname):\n",
    "    data = data.reshape(-1,3)\n",
    "    points = set()\n",
    "    for d in data:\n",
    "        points.add(tuple(d))\n",
    "    vertexes = list(points)\n",
    "    print len(vertexes),len(data)\n",
    "    faces = []\n",
    "    for d in data:\n",
    "        faces.append(vertexes.index(tuple(d)))\n",
    "    faces = np.array(faces)\n",
    "    faces = faces.reshape(-1,3)\n",
    "    with open(fname,'w') as openfile:\n",
    "        openfile.write(\"OFF\\n\")\n",
    "        openfile.write(\"{} {} 0\\n\".format(len(vertexes),len(faces)))\n",
    "        for v in vertexes:\n",
    "            openfile.write(\"{} {} {}\\n\".format(v[0],v[1],v[2]))\n",
    "        for f in faces:\n",
    "            openfile.write(\"3 {} {} {}\\n\".format(f[0],f[1],f[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
