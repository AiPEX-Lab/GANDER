{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7fdd7b579290>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "from torch import optim\n",
    "from torch.nn import functional as F \n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [\"airplane\",\"bench\",\"bowl\",\"cone\",\"desk\",\"flower_pot\",\n",
    "         \"keyboard\",\"mantel\",\"person\",\"radio\",\"sofa\",\"table\",\n",
    "         \"tv_stand\",\"xbox\",\"bathtub\",\"bookshelf\",\"car\",\"cup\",\n",
    "         \"door\",\"glass_box\",\"lamp\",\"monitor\",\"piano\",\"range_hood\",\n",
    "         \"stairs\",\"tent\",\"vase\",\"bed\",\"bottle\",\"chair\",\"curtain\",\n",
    "         \"dresser\",\"guitar\",\"laptop\",\"night_stand\",\"plant\",\n",
    "         \"sink\",\"stool\",\"toilet\",\"wardrobe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_data(root_directory):\n",
    "    fixed = []\n",
    "    df = pd.read_pickle(\"singles.pkl\")\n",
    "    for i in range(len(df)):\n",
    "        fname = df.iloc[i]['name']\n",
    "        ftype = \"_\".join(fname.split(\"/\")[-1].split(\"_\")[:-1])\n",
    "        fno = fname.split(\"_\")[-1][:-4]\n",
    "        fname = \"{}{}_{}_0.pkl\".format(root_directory,ftype,fno)\n",
    "        try:\n",
    "            faces = pickle.load(open(fname,'rb'))\n",
    "            entry = [fname,ftype,len(faces)]\n",
    "            fixed.append(entry)\n",
    "        except IOError:\n",
    "            continue\n",
    "    return fixed\n",
    "#f = fix_data(\"../CleanedModels/pickle/\")\n",
    "#df = pd.DataFrame(f,columns=[\"name\",\"cls_name\",\"sizes\"])\n",
    "#df.to_pickle(\"singles.fixed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "class MeshSampler(Sampler):\n",
    "    def __init__(self,dataset,batch_size=32):\n",
    "        self.epoch_size = len(dataset)\n",
    "        self.batch_size = batch_size\n",
    "        self.sample_ind = range(len(dataset))\n",
    "    def __len__(self):\n",
    "        return (self.epoch_size + self.batch_size - 1) // self.batch_size\n",
    "    def __iter__(self):\n",
    "        order = range(int(self.epoch_size/self.batch_size))\n",
    "        shuffle(order)\n",
    "        for i in order:\n",
    "            start = i * self.batch_size\n",
    "            end = min(start + self.batch_size, self.epoch_size)\n",
    "            yield self.sample_ind[start:end]\n",
    "from numpy import cross, eye, dot\n",
    "from scipy.linalg import expm, norm\n",
    "\n",
    "def renorm(data):\n",
    "    print data.shape\n",
    "    med = (np.max(data,axis=0) + np.min(data,axis=0))/2\n",
    "    data -= med\n",
    "    print med,np.max(np.linalg.norm(data,axis=1))\n",
    "    data /= np.max(np.linalg.norm(data,axis=1))\n",
    "    return data\n",
    "\n",
    "def M(axis, theta):\n",
    "    return expm(cross(eye(3), axis/norm(axis)*theta))\n",
    "def augment(faces,theta,rotate):\n",
    "    theta = theta*30*np.pi/180.\n",
    "    if rotate and theta > 0:\n",
    "        R = M([0,0,1],theta)\n",
    "        faces = np.dot(faces,R)\n",
    "    return faces\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "class MeshData(Dataset):\n",
    "    def __init__(self,root_directory, class_we_want,max_vertices=5000):\n",
    "        print \"loading...\"\n",
    "        self.root = root_directory\n",
    "        self.df = pd.read_pickle(\"sizes.pkl\")\n",
    "        print \"done\"\n",
    "        self.df = self.df[(self.df.sizev<max_vertices) &\n",
    "                          (self.df.cls_name==class_we_want) & \n",
    "                            (self.df.fname.str.contains(\"_0_\"))]#.sort_values(\"sizef\",ascending=False)\n",
    "        self.max_vertexes = np.max(self.df.sizev.values)\n",
    "    def __len__(self):\n",
    "        return len(self.df)*12\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx//12\n",
    "        theta = idx%12\n",
    "        fname = self.df.iloc[i]['fname']\n",
    "        end = fname.split(\"/\")[-1]\n",
    "        ftype = \"_\".join(fname.split(\"/\")[-1].split(\"_\")[:-4])\n",
    "        fno = fname.split(\"_\")[-1][:-4]\n",
    "        #fname = \"{}{}_{}_0.pkl\".format(self.root,ftype,fno)\n",
    "        data = pickle.load(open(fname,'rb'))\n",
    "        vertices = data['vertices']\n",
    "        vertices = augment(vertices,theta,True)\n",
    "        assert len(vertices) <= self.max_vertexes\n",
    "        return torch.from_numpy(vertices).float()\n",
    "trainer = MeshData(\"../CleanedModels/pickle/\",\"chair\",300)\n",
    "\n",
    "def fill_with_zeros(x,l):\n",
    "    new_x = torch.zeros(l)\n",
    "    new_x[:x.size(0)]=x\n",
    "    return new_x\n",
    "    \n",
    "def pad_packer(x):\n",
    "    faces,cls_idx = zip(*x)\n",
    "    cls_idx = torch.cat(cls_idx)\n",
    "    max_size = faces[0].size()\n",
    "    lengths = [len(i) for i in faces]\n",
    "    faces = [fill_with_zeros(i,max_size) for i in faces]\n",
    "    faces = Variable(torch.stack(faces,0))\n",
    "    return faces,cls_idx,lengths\n",
    "train_loader = DataLoader(trainer,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def loss_function(output, faces, mu, logvar):\n",
    "    recon_x = output\n",
    "    x = faces\n",
    "    CD = ChamferDistance(recon_x,x)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    return CD,KLD\n",
    "\n",
    "def pairwise_dist(x, y):\n",
    "    xx, yy, zz = torch.mm(x,x.t()), torch.mm(y,y.t()), torch.mm(x, y.t())\n",
    "    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n",
    "    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n",
    "    P = (rx.t() + ry - 2*zz)\n",
    "    return P\n",
    "\n",
    "\n",
    "def ChamferDistance(x, y, dim=0):\n",
    "    x,y = x.squeeze(),y.squeeze()\n",
    "    #print x.size(),y.size()\n",
    "    dist = pairwise_dist(x, y)\n",
    "    #i,j = np.random.randint(2000),np.random.randint(2000)\n",
    "    #print torch.sum(torch.pow(x[i]-y[j],2)),dist[i,j]\n",
    "    xvalues, _ = dist.min(dim=dim)\n",
    "    yvalues, _ = dist.min(dim=1)\n",
    "    return xvalues.mean()+yvalues.mean()\n",
    "\n",
    "#print ChamferDistance(a.cuda(),b.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('LSTM') != -1:\n",
    "        nn.init.xavier_normal(m.weight_ih_l0.data)\n",
    "        nn.init.xavier_normal(m.weight_hh_l0.data)\n",
    "        nn.init.constant(m.bias_ih_l0.data,0.001)\n",
    "        nn.init.constant(m.bias_hh_l0.data,0.001)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal(m.weight.data)\n",
    "        nn.init.constant(m.bias.data,0.001)\n",
    "class VertexEncoder(nn.Module):\n",
    "    def __init__(self,lstm_units,z_dim,in_dims=3,bi=True):\n",
    "        super(VertexEncoder, self).__init__()\n",
    "        self.hidden_size=lstm_units\n",
    "        self.bi = bi\n",
    "        if bi:\n",
    "            self.hidden_size *= 2\n",
    "        self.lstm = nn.LSTM(in_dims,lstm_units,1,batch_first=True,bidirectional=bi)\n",
    "        self.mu = nn.Linear(self.hidden_size,z_dim)\n",
    "        self.logvar = nn.Linear(self.hidden_size,z_dim)\n",
    "    def forward(self,x,h):\n",
    "        x,(h,c) = self.lstm(x,h)\n",
    "        h = F.relu(h.squeeze()).view(self.hidden_size)\n",
    "        mu = self.mu(h)\n",
    "        logvar = self.logvar(h)\n",
    "        return mu,logvar\n",
    "    def initHidden(self):\n",
    "        firstdim = 1\n",
    "        if self.bi:\n",
    "            firstdim = 2\n",
    "        h0 = Variable(torch.zeros(firstdim, 1, self.hidden_size/firstdim))\n",
    "        c0 = Variable(torch.zeros(firstdim, 1, self.hidden_size/firstdim))\n",
    "        if use_cuda:\n",
    "            return h0.cuda(),c0.cuda()\n",
    "        else:\n",
    "            return h0,c0\n",
    "class VertexDecoder(nn.Module):\n",
    "    def __init__(self,lstm_units,z_dim,in_dims=3):\n",
    "        super(VertexDecoder, self).__init__()\n",
    "        self.hidden_size=lstm_units\n",
    "        self.z_dim = z_dim\n",
    "        self.lstm = nn.LSTM(self.z_dim+in_dims,self.hidden_size)\n",
    "        self.fc1 = nn.Linear(self.hidden_size,in_dims)\n",
    "        self.fc_hc = nn.Linear(self.z_dim, 2*self.hidden_size)\n",
    "    def forward(self,inputs,z,seq_len,hidden_cell=None):\n",
    "        if hidden_cell is None:\n",
    "            h = F.tanh(self.fc_hc(z))\n",
    "            hidden,cell = torch.split(h,self.hidden_size,1)\n",
    "            hidden_cell = (hidden.unsqueeze(0).contiguous(), cell.unsqueeze(0).contiguous())\n",
    "        outputs,(hidden,cell) = self.lstm(inputs, hidden_cell)\n",
    "        outputs = self.fc1(outputs)\n",
    "        return outputs, (hidden,cell)\n",
    "\n",
    "class VertexVAE(nn.Module):\n",
    "    def __init__(self,z_dim=64):\n",
    "        super(VertexVAE, self).__init__()\n",
    "        self.encoder = VertexEncoder(lstm_units=200,z_dim=z_dim)\n",
    "        self.decoder = VertexDecoder(lstm_units=400,z_dim=z_dim)\n",
    "    def forward(self,x,h):\n",
    "        seq_len = x.size(1)\n",
    "        mu,logvar = self.encoder(x,h)\n",
    "        z = self.reparameterize(mu,logvar)\n",
    "        z_stack = torch.stack([z]*(seq_len)).unsqueeze(0)\n",
    "        z = z.unsqueeze(0)\n",
    "        inputs = torch.cat([x, z_stack],2).t()\n",
    "        x,_ = self.decoder(inputs,z,seq_len)\n",
    "        return x, mu, logvar, z\n",
    "    def reparameterize(self, mu, sigma):\n",
    "        if self.training:\n",
    "            std = sigma.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vvae = VertexVAE()\n",
    "vvae.apply(weights_init)\n",
    "if use_cuda:\n",
    "    vvae = vvae.cuda()     \n",
    "optimizer = optim.Adam(vvae.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard things\n",
    "writer = SummaryWriter()\n",
    "global_counter = 0 \n",
    "\n",
    "#vandf.load_state_dict(torch.load(\"facer.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_tensors(old_tensors):\n",
    "    tensors_existing = []\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            if (hasattr(obj, 'data')):\n",
    "                obj = obj.data\n",
    "            if \"cuda\" in str(type(obj)):\n",
    "                #print type(obj)\n",
    "                tensors_existing.append(obj)\n",
    "    if old_tensors is not None:\n",
    "        found = 0\n",
    "        for te in tensors_existing:\n",
    "            foundit = False\n",
    "            for o in old_tensors:\n",
    "                if o.type()==te.type() and o.size()==te.size() and torch.equal(o,te):\n",
    "                    found+=1\n",
    "                    foundit = True\n",
    "                    break\n",
    "            if not foundit:\n",
    "                print te.size()\n",
    "        for o in old_tensors:\n",
    "            foundit = False\n",
    "            for te in tensors_existing:\n",
    "                if o.type()==te.type() and o.size()==te.size() and torch.equal(o,te):\n",
    "                    foundit = True\n",
    "                    break\n",
    "            if not foundit:\n",
    "                print \"deleted one\"\n",
    "            \n",
    "                \n",
    "        return tensors_existing,found\n",
    "    return tensors_existing,0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't delete () (<ipython-input-10-c26e4287484c>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-c26e4287484c>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    del()\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't delete ()\n"
     ]
    }
   ],
   "source": [
    "# VAE only\n",
    "printevery = 3\n",
    "import gc\n",
    "te,found = log_tensors(None)\n",
    "print \"starting tensors: {}\".format(len(te))\n",
    "#ms2s.cuda()\n",
    "genevery = 100\n",
    "import time\n",
    "for i in range(30):\n",
    "    for idx,(vertices) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        vertices = Variable(vertices)\n",
    "        if use_cuda:\n",
    "            #faces=faces.cuda()\n",
    "            vertices=vertices.cuda()\n",
    "        #print faces.size()\n",
    "        seq_len = vertices.size(1)\n",
    "        print seq_len\n",
    "        writer.add_scalars(\"sizes\",{\"vlen\":seq_len},global_step=global_counter)\n",
    "        te,found = log_tensors(te)\n",
    "        print \"tensors before forward: {} {}\".format(len(te),found)\n",
    "        now = time.time()\n",
    "        h = vvae.encoder.initHidden()\n",
    "        out_vertexes,mu,logvar,z = vvae(vertices,h)\n",
    "        forward_time = time.time()-now\n",
    "        CD,KLD = loss_function(out_vertexes,vertices,mu,logvar)\n",
    "        now = time.time()\n",
    "        loss = CD + KLD\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(vvae.parameters(), 1.)\n",
    "        optimizer.step()\n",
    "        te,found = log_tensors(te)\n",
    "        print \"tensors after forward: {} {}\".format(len(te),found)\n",
    "        backward_time = time.time()-now\n",
    "        writer.add_scalars(\"timing/master\",{\"forward\": forward_time, \"backward\": backward_time},global_counter)\n",
    "        #print \"forward: {}, backward: {}\".format(forward_time,backward_time)\n",
    "        writer.add_scalars(\"losses\",{\"CD_Vertex\": CD.data.cpu()[0],\n",
    "                                     \"KLD_Vertex\": KLD.data.cpu()[0],\n",
    "                                    \"total\":loss},global_counter)\n",
    "        te,found = log_tensors(te)\n",
    "        print \"tensors after logging: {} {}\".format(len(te),found)\n",
    "        global_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(vandf.cpu().state_dict(),\"facer.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def write_off(data,fname):\n",
    "    data = data.reshape(-1,3)\n",
    "    points = set()\n",
    "    for d in data:\n",
    "        points.add(tuple(d))\n",
    "    vertexes = list(points)\n",
    "    print len(vertexes),len(data)\n",
    "    faces = []\n",
    "    for d in data:\n",
    "        faces.append(vertexes.index(tuple(d)))\n",
    "    faces = np.array(faces)\n",
    "    faces = faces.reshape(-1,3)\n",
    "    with open(fname,'w') as openfile:\n",
    "        openfile.write(\"OFF\\n\")\n",
    "        openfile.write(\"{} {} 0\\n\".format(len(vertexes),len(faces)))\n",
    "        for v in vertexes:\n",
    "            openfile.write(\"{} {} {}\\n\".format(v[0],v[1],v[2]))\n",
    "        for f in faces:\n",
    "            openfile.write(\"3 {} {} {}\\n\".format(f[0],f[1],f[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
